{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d12d93e45e45a37f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from numerapi import NumerAPI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:09:11.392634Z",
     "start_time": "2024-04-02T14:09:10.491139Z"
    }
   },
   "id": "2472c8636a2eac0a",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4dd28c51a881eb6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 16:09:12,876 INFO numerapi.utils: starting download\n",
      "data/v4.3/train_int8.parquet: 2.09GB [04:54, 7.11MB/s]                                \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m local_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m, remote_file)\n\u001B[1;32m      5\u001B[0m napi\u001B[38;5;241m.\u001B[39mdownload_dataset(remote_file, dest_path\u001B[38;5;241m=\u001B[39mlocal_file)\n\u001B[0;32m----> 6\u001B[0m training_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/notebooks-FzXm3nz_-py3.12/lib/python3.12/site-packages/pandas/io/parquet.py:651\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001B[0m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;129m@doc\u001B[39m(storage_options\u001B[38;5;241m=\u001B[39m_shared_docs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_parquet\u001B[39m(\n\u001B[1;32m    500\u001B[0m     path: FilePath \u001B[38;5;241m|\u001B[39m ReadBuffer[\u001B[38;5;28mbytes\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    508\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    509\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m    510\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001B[39;00m\n\u001B[1;32m    512\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;124;03m    1    4    9\u001B[39;00m\n\u001B[1;32m    649\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 651\u001B[0m     impl \u001B[38;5;241m=\u001B[39m \u001B[43mget_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    653\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_nullable_dtypes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mno_default:\n\u001B[1;32m    654\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    655\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_nullable_dtypes\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is deprecated and will be removed \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    656\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min a future version.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    657\u001B[0m         )\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/notebooks-FzXm3nz_-py3.12/lib/python3.12/site-packages/pandas/io/parquet.py:67\u001B[0m, in \u001B[0;36mget_engine\u001B[0;34m(engine)\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m     65\u001B[0m             error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(err)\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to find a usable engine; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtried using: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfastparquet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA suitable version of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyarrow or fastparquet is required for parquet \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msupport.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     73\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying to import the above resulted in these errors:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_msgs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m     )\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m engine \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m PyArrowImpl()\n",
      "\u001B[0;31mImportError\u001B[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "# Use int8 to save on storage and memory\n",
    "remote_file = \"v4.3/train_int8.parquet\"\n",
    "local_file = os.path.join(\"data\", remote_file)\n",
    "napi.download_dataset(remote_file, dest_path=local_file)\n",
    "training_data = pd.read_parquet(local_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:14:07.556479Z",
     "start_time": "2024-04-02T14:09:11.393567Z"
    }
   },
   "id": "966ca43b848d5459",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Template Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c163dce44411bd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "features = [f for f in training_data.columns if \"feature\" in f]\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    num_leaves=2 ** 5,\n",
    "    colsample_bytree=0.1\n",
    ")\n",
    "model.fit(\n",
    "    training_data[features],\n",
    "    training_data[\"target\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63fe643489961e47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submit forecast"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd12103680c6386"
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the docs on https://docs.numer.ai"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2281b41523bc02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "93dbee6f6c7dfdde"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
