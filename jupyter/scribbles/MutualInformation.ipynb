{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ad7d70-d88d-4975-9611-1d069560a135",
   "metadata": {},
   "source": [
    "# Mutual Information\n",
    "\n",
    "In this notebook I want to visualize how mutual information varies with a joint distribution of discrete variables. The variables are specific to a side project, where different statements will be assigned one of 4 stances by each user:\n",
    "1. True\n",
    "2. False\n",
    "3. Unsure\n",
    "4. Vague\n",
    "\n",
    "Together with a 5th stance ( unanswered ), we can build informational metrics around it, namely [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "\n",
    "Key relations:\n",
    "- $I(X,Y) = H(X) + H(Y) - H(X,Y)$\n",
    "- $H(X | Y ) = H(X,Y) - H(Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa7eca9-6962-4d4b-b894-48b3700defc0",
   "metadata": {},
   "source": [
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "mp = pprint.PrettyPrinter(indent=2).pprint"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6895ec70-fbe2-49d6-8238-c04c268535ec",
   "metadata": {},
   "source": [
    "# Colour pallette\n",
    "sns.color_palette(\"viridis\", as_cmap=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d2f31f-c4ac-4ee9-8238-725e1bb60641",
   "metadata": {},
   "source": [
    "stances = [\"True\", \"False\", \"Unsure\", \"Vague\", \"Unanswered\"]\n",
    "\n",
    "\n",
    "class JointDistribution:\n",
    "    def __init__(self, data=None, baseline=1):\n",
    "        if data:\n",
    "            data = [\n",
    "                (a, b, baseline + random.randint(0, 100))\n",
    "                for a, b in itertools.product(stances, repeat=2)\n",
    "            ]\n",
    "        self.votes = pd.DataFrame(\n",
    "            data=data,\n",
    "            columns=[\"statement_a\", \"statement_b\", \"votes\"],\n",
    "        )\n",
    "        self.entropies = {\n",
    "            \"a\": entropy(\n",
    "                list(self.votes.groupby(\"statement_a\").sum()[\"votes\"]), base=2\n",
    "            ),\n",
    "            \"b\": entropy(\n",
    "                list(self.votes.groupby(\"statement_b\").sum()[\"votes\"]), base=2\n",
    "            ),\n",
    "            \"total\": entropy(self.votes[\"votes\"], base=2),\n",
    "        }\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        sns.heatmap(\n",
    "            data=self.votes.pivot(\n",
    "                index=\"statement_a\", columns=\"statement_b\", values=\"votes\"\n",
    "            ),\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "\n",
    "    def print_information_summary(self):\n",
    "        mutual_information = {\n",
    "            \"mutual\": self.entropies[\"a\"]\n",
    "            + self.entropies[\"b\"]\n",
    "            - self.entropies[\"total\"]\n",
    "        }\n",
    "        mp(self.entropies | mutual_information)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_flat_array(flat_array):\n",
    "        return JointDistribution(\n",
    "            data=[\n",
    "                (a, b, v)\n",
    "                for (a, b), v in zip(itertools.product(stances, repeat=2), flat_array)\n",
    "            ]\n",
    "        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de56e5a-d552-4d9d-92a4-6ae83ae0178d",
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "math.log(5, 2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097b93be-2bb5-4dbe-b504-4d7f96de16e3",
   "metadata": {},
   "source": [
    "baseline = 0.2\n",
    "jd = JointDistribution(\n",
    "    data=[\n",
    "        (a, b, 10**7 if a == b else baseline)\n",
    "        for a, b in itertools.product(stances, repeat=2)\n",
    "    ]\n",
    ")\n",
    "jd.print_information_summary()\n",
    "jd.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7eb86c-b049-4d89-a95a-ad790daa5439",
   "metadata": {},
   "source": [
    "jd.votes.pivot(index=\"statement_a\", columns=\"statement_b\", values=\"votes\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "15cc726a-d6de-4152-89b0-1277ab138e23",
   "metadata": {},
   "source": [
    "## Mini problem\n",
    "\n",
    "I want to find the joint distribution that minimizes $H(X|Y)/H(Y|X)$. We will do this by setting a length 25 vector, and search over this space, using the ratio below as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c889864-aa50-4899-b9cc-0349ce600acc",
   "metadata": {},
   "source": [
    "def loss_function(vector: np.ndarray) -> float:\n",
    "    jd = JointDistribution.from_flat_array(vector)\n",
    "    return np.divide(\n",
    "        jd.entropies[\"total\"] - jd.entropies[\"b\"],\n",
    "        jd.entropies[\"total\"] - jd.entropies[\"a\"],\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335c8901-9dcd-4916-8ad9-26e848372bfd",
   "metadata": {},
   "source": [
    "x = minimize(\n",
    "    x0=[random.randint(1, 5) for _ in range(25)],\n",
    "    # x0=x.x,\n",
    "    fun=loss_function,\n",
    "    method=\"SLSQP\",\n",
    "    constraints={\n",
    "        LinearConstraint(\n",
    "            A=np.diag(np.ones(25)), ub=np.divide(np.ones(25), 0.01), lb=np.ones(25)\n",
    "        )\n",
    "    },\n",
    ")\n",
    "minimal_jd = JointDistribution.from_flat_array(x.x)\n",
    "minimal_jd.print_information_summary()\n",
    "minimal_jd.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "16d60ad7-3447-492f-ab76-26b0f3586944",
   "metadata": {},
   "source": [
    "From this experiment it seems like the ratio is minimized when we roughly have a uniform distribution on the possible pairs, but we make the entropy of a minimal by only allowing 2 values, and the entropy of b maximal by allowing all values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
